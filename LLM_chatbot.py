pip install pandas faiss-cpu sentence-transformers langchain google-generativeai
pip install -U langchain-community
import pandas as pd
import faiss
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import os

# ë³¸ì¸ í‚¤ë¡œ ë°”ê¿”ì„œ ì…ë ¥
os.environ["GOOGLE_API_KEY"] = "---"

from langchain.embeddings import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# ìƒ˜í”Œ ë¬¸ì¥ ì„ë² ë”©
texts = ["ì´ ì±… ì •ë§ ì¬ë¯¸ìˆì–´ìš”.", "ë²ˆì—­ì´ ì¡°ê¸ˆ ì•„ì‰¬ì› ì–´ìš”."]
embeddings = embedding.embed_documents(texts)

print("ì„ë² ë”© ì°¨ì›:", len(embeddings[0]))
print("ì„ë² ë”© í™•ì¸:", embeddings[0][:5])

import pandas as pd
import re
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# ğŸ“¥ 1. CSV ë¡œë”© ë° í•©ì¹˜ê¸°
df1 = pd.read_csv("/content/drive/MyDrive/ìƒˆí´ë”1/Dataset/preprocessed_reviews_yes24.csv")
df2 = pd.read_csv("/content/drive/MyDrive/ìƒˆí´ë”1/Dataset/preprocessed_reviews_kyobo.csv")
df3 = pd.read_csv("/content/drive/MyDrive/ìƒˆí´ë”1/Dataset/preprocessed_reviews_aladin.csv")
df = pd.concat([df1, df2, df3], ignore_index=True)

# ğŸ§¼ 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def clean_text(text):
    text = str(text).strip()
    text = re.sub(r"\s+", " ", text)              # ê³µë°± ì •ë¦¬
    text = re.sub(r"[^\w\s.,!?ê°€-í£]", "", text)   # íŠ¹ìˆ˜ê¸°í˜¸ ì œê±° (ì´ëª¨ì§€ ë“±)
    return text

# ğŸ¯ 3. review ì»¬ëŸ¼ë§Œ ì‚¬ìš© + ì „ì²˜ë¦¬
df = df.dropna(subset=["review"])  # ê²°ì¸¡ì¹˜ ì œê±°
df["review"] = df["review"].apply(clean_text)
df = df[df["review"].str.len() >= 10]  # ë„ˆë¬´ ì§§ì€ ë¦¬ë·° ì œê±°
df = df.drop_duplicates(subset=["review"])  # ì¤‘ë³µ ì œê±°

# ğŸ“„ 4. Documentë¡œ ë³€í™˜
documents = [
    Document(page_content=f"[ë¦¬ë·°] {row['review']}")
    for _, row in df.iterrows()
]
print(f"ğŸ“„ ì „ì²˜ë¦¬ í›„ {len(documents)}ê°œì˜ ë¦¬ë·° ì‚¬ìš©ë¨")

# âœ‚ï¸ 5. Chunking
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)
print(f"âœ‚ï¸ ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")

# ğŸ§  6. ì„ë² ë”© ë° FAISS ì¸ë±ìŠ¤ ì €ì¥
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectordb = FAISS.from_documents(chunks, embedding)

save_path = "faiss_index_test"
vectordb.save_local(save_path)
print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {save_path}/")

!pip install --upgrade langchain
!pip install langchain-google-genai

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA

# âœ… 1. FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectordb = FAISS.load_local(
    "faiss_index_test",
    embedding,
    allow_dangerous_deserialization=True
)


# âœ… 2. Gemini LLM ì„¸íŒ…
llm = ChatGoogleGenerativeAI(
    model="models/gemini-2.0-flash",
    temperature=0.7
)

# âœ… 3. RAG QA ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True  # (ì„ íƒ) ê·¼ê±°ë„ ê°™ì´ ë³´ê¸° ìœ„í•´
)
# âœ… 4. í…ŒìŠ¤íŠ¸ ì§ˆì˜
query = "ë¦¬ë·°ì—ì„œ ê°ë™, ìŠ¬í”” ì¤‘ ì–´ë–¤ ê°ì •ì´ ë” ë§ì´ ì–¸ê¸‰ëì–´?"
result = qa_chain({"query": query})

# âœ… 5. ê²°ê³¼ ì¶œë ¥
print("ğŸ’¬ Gemini ì‘ë‹µ:\n")
print(result["result"])

# (ì„ íƒ) ì–´ë–¤ ë¦¬ë·°ë“¤ì´ ê·¼ê±°ë¡œ ì“°ì˜€ëŠ”ì§€ í™•ì¸
print("\nğŸ“„ ì°¸ê³ ëœ ë¦¬ë·°ë“¤:")
for doc in result["source_documents"]:
    print(" -", doc.page_content[:100], "...")

#----------------------------ì—¬ê¸°ê¹Œì§€ RAG LLM -------------------------

# st_app/graph/nodes/subject_info_node.py

import json
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

# âœ… 1. JSON íŒŒì¼ ë¡œë”©
with open("/content/drive/MyDrive/ìƒˆí´ë”1/Dataset/subjects.json", "r", encoding="utf-8") as f:
    subject_data = json.load(f)

# âœ… 2. LLM ì´ˆê¸°í™” (Gemini Flash ì‚¬ìš©)
llm = ChatGoogleGenerativeAI(model="models/gemini-2.0-flash", temperature=0.7)

# âœ… 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì±…ì˜ ì •ë³´ë¥¼ ìš”ì•½í•´ì„œ ì‚¬ìš©ìì—ê²Œ ì„¤ëª…í•˜ëŠ” ì±… ì†Œê°œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì€ ì±… ì†Œê°œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{query}"

ì•„ë˜ëŠ” '{title}'ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤:

{book_info}

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ì¶° í•„ìš”í•œ ì •ë³´ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
""")

# âœ… 4. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
def subject_info_node(state):
    query = state["user_input"]
    book_title = "ì†Œë…„ì´ ì˜¨ë‹¤"  # (ì˜ˆì‹œ: ì‹¤ì œ ì‚¬ìš© ì‹œì—” LLMìœ¼ë¡œ ì¶”ì¶œí•˜ê±°ë‚˜ keyword ë§¤ì¹­)

    if subject_data["title"] != book_title:
        return {"user_input": "", "chat_history": state["chat_history"] + [(query, "ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]}

    # ì±… ì •ë³´ í…ìŠ¤íŠ¸ êµ¬ì„±
    book_info = json.dumps(subject_data, ensure_ascii=False, indent=2)

    # í”„ë¡¬í”„íŠ¸ ì±„ìš°ê¸°
    prompt = template.format_messages(query=query, title=book_title, book_info=book_info)

    # Gemini ì‘ë‹µ ìƒì„±
    response = llm(prompt).content

    return {
        "user_input": "",
        "chat_history": state["chat_history"] + [(query, response)]
    }
state = {
    "user_input": "ì†Œë…„ì´ ì˜¨ë‹¤ ì¶œíŒì‚¬ ì•Œë ¤ì¤˜",
    "chat_history": []
}

result = subject_info_node(state)
print(result["chat_history"][-1])

#------------------ì—¬ê¸°ê¹Œì§€ subject LLM--------------
